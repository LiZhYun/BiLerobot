# Demonstrations

High-quality demonstration datasets are one of the features of ManiSkill2. Most demonstrations are generated by motion planning with privileged information. Some demonstrations are generated by [model predictive control](https://en.wikipedia.org/wiki/Model_predictive_control) (MPC) or state-based Reinforcement Learning (RL) given our dense rewards.

## Download

We provide a command line tool (`mani_skill2.utils.download_demo`) to download demonstrations from Google Drive. The full datasets are avaiable at [Google Drive](https://drive.google.com/drive/folders/1hVdUNPGCHh0OULPCowBClPYIXSwsx-J9). Please refer to [Environments](../concepts/environments.md) for all supported environments. Please see our [notes](https://docs.google.com/document/d/1bBKmsR-R_7tR9LwaT1c3J26SjIWw27tWSLdHnfBR01c/edit?usp=sharing) about the details of demonstrations.

```bash
# Download the full datasets
python -m mani_skill2.utils.download_demo all
# Download the demonstration dataset for certain task
python -m mani_skill2.utils.download_demo ${ENV_ID}
```

For those who cannot access Google Drive, the datasets can be downloaded from [ScienceDB.cn](http://doi.org/10.57760/sciencedb.02239).

## Format

All demonstrations for an environment are saved in the HDF5 format. Each HDF5 dataset is named `trajectory.{obs_mode}.{control_mode}.h5`, and is associated with a JSON file with the same base name. The JSON file stores meta information. Unless otherwise specified, `trajectory.h5` is short for `trajectory.none.pd_joint_pos.h5`, which contains the original demonstrations generated by the `pd_joint_pos` controller with the `none` observation mode (empty observations). However, there may exist demonstrations generated by other controllers. **Thus, please check the associated JSON to ensure which controller is used.**

:::{note}
For `PickSingleYCB-v0`, `TurnFaucet-v0`, the dataset is named `{model_id}.h5` for each asset. It is due to some legacy issues, and might be changed in the future.

For `OpenCabinetDoor-v1`, `OpenCabinetDrawer-v1`, `PushChair-v1`, `MoveBucket-v1`, which are migrated from [ManiSkill1](https://github.com/haosulab/ManiSkill), trajectories are generated by the RL and `base_pd_joint_vel_arm_pd_joint_vel` controller.
:::

### Meta Information (JSON)

Each JSON file contains:

- env_info (`Dict`): environment information, which can be used to initialize the environment
  - env_id: environment id
  - max_episode_steps
  - env_kwargs: keyword arguments to initialize the environment. **Essential to reproduce the trajectory.**
- episodes (`List[Dict]`): episode information

The episode information (the element of `episodes`) includes:

- episode_id: a unique id to index the episode
- reset_kwargs: keyword arguments to reset the environment. **Essential to reproduce the trajectory.**
- control_mode: control mode used for the episode.
- elapsed_steps: trajectory length
- info: information at the end of the episode.

To reproduce the environment for the trajectory:

```python
env = gym.make(env_info["env_id"], **env_info["env_kwargs"])
episode = env_info["episodes"][0]
env.reset(**episode["reset_kwargs"])
```

### Trajectory Data (HDF5)

Each HDF5 demonstration dataset consists of multiple trajectories. The key of each trajectory is `traj_{episode_id}`, e.g., `traj_0`.

Each trajectory is an `h5py.Group`, which contains:

- actions: [T, A], `np.float32`. `T` is the number of transitions.
- success: [T], `np.bool_`. It indicates whether the task is successful at each time step.
- env_states: [T+1, D], `np.float32`. Environment states. It can be used to set the environment to a certain state, e.g., `env.set_state(env_states[i])`. However, it may not be enough to reproduce the trajectory.
- env_init_state: [D], `np.float32`. The initial environment state. It is used for soft-body environments, since their states (particle positions) can use too much space.
- obs (optional): observations. If the observation is a `dict`, the value will be stored in `obs/{key}`. The convention is applied recursively for nested dict.
